{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"27ce2e177561428aaa4053cbe782886e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f16c9aa8f684f60877e5eb5973a33a3","IPY_MODEL_077d17bb2547490d9f16d39fccc5751b","IPY_MODEL_ae4d09a4ef5e42158cbab5e25fbdce3f"],"layout":"IPY_MODEL_3ded05a4af3341248d90e23414ee3b26"}},"9f16c9aa8f684f60877e5eb5973a33a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f6281f4e3b94fcc8a9b2f56565bf867","placeholder":"​","style":"IPY_MODEL_dc138644816341f6ae3e1b30bb0d9fd8","value":"Epoch 1 of 1: 100%"}},"077d17bb2547490d9f16d39fccc5751b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc5cdb20a36640eeaa341272da1f713c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de18c25820dd41bc96a06ab059613010","value":1}},"ae4d09a4ef5e42158cbab5e25fbdce3f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fadcd5d8f1504975bfcaa0e3faf2fe93","placeholder":"​","style":"IPY_MODEL_2be045325e2c4624bc178b1ddb4dfefc","value":" 1/1 [07:40&lt;00:00, 460.29s/it]"}},"3ded05a4af3341248d90e23414ee3b26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f6281f4e3b94fcc8a9b2f56565bf867":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc138644816341f6ae3e1b30bb0d9fd8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc5cdb20a36640eeaa341272da1f713c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de18c25820dd41bc96a06ab059613010":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fadcd5d8f1504975bfcaa0e3faf2fe93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2be045325e2c4624bc178b1ddb4dfefc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07fc26186fa94a45ac7f269038456246":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ffc5d8ba5ae7402fa86526c3cccbe445","IPY_MODEL_d7971d14341142289024e63b77d775d1","IPY_MODEL_f3fcae8c2d3d46f8a1afaf888169c897"],"layout":"IPY_MODEL_7726141bc8314fa891b68d76a3bc125d"}},"ffc5d8ba5ae7402fa86526c3cccbe445":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b9c1e9d980a4cc2bd375cab6ac77454","placeholder":"​","style":"IPY_MODEL_5f0f0a1dfe384b57befa5331b5c37f40","value":"Epochs 1/1. Running Loss:    0.7639: 100%"}},"d7971d14341142289024e63b77d775d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_725226bbf4ac45e59df88ad30e632ee9","max":741,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b3b9d3edfbe14dbe98ade9db4449fff4","value":741}},"f3fcae8c2d3d46f8a1afaf888169c897":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c99f971bf3b446d97f70abe8fbd38c0","placeholder":"​","style":"IPY_MODEL_60afecf161c34933970e7df94400f22e","value":" 741/741 [07:08&lt;00:00,  1.69it/s]"}},"7726141bc8314fa891b68d76a3bc125d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b9c1e9d980a4cc2bd375cab6ac77454":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f0f0a1dfe384b57befa5331b5c37f40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"725226bbf4ac45e59df88ad30e632ee9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3b9d3edfbe14dbe98ade9db4449fff4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1c99f971bf3b446d97f70abe8fbd38c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60afecf161c34933970e7df94400f22e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71683651c71b4faa865169677d6dd807":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc7011052397495492683dffcc2c23d3","IPY_MODEL_8fdc27bbbda94cc694c3c2d8d4203d37","IPY_MODEL_8f79b79bffe64ba79b0e06878589f9ff"],"layout":"IPY_MODEL_b0d973fb40ce4b1e992583d8bca11844"}},"bc7011052397495492683dffcc2c23d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9df2536af775494aa08e3a8241275d28","placeholder":"​","style":"IPY_MODEL_0a2f6da832d94dc3b5da7a7f25bc9ab9","value":"Running Evaluation: 100%"}},"8fdc27bbbda94cc694c3c2d8d4203d37":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dd7f67761474048aa9e30ce86d5520c","max":1355,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16ecbddd6399447eb0369c1b612e1cc9","value":1355}},"8f79b79bffe64ba79b0e06878589f9ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a55e6c8ee31441199c87b0df944334b6","placeholder":"​","style":"IPY_MODEL_0e29aed54f914bf5ac7401a8fe36fe11","value":" 1355/1355 [04:04&lt;00:00,  5.57it/s]"}},"b0d973fb40ce4b1e992583d8bca11844":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9df2536af775494aa08e3a8241275d28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a2f6da832d94dc3b5da7a7f25bc9ab9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0dd7f67761474048aa9e30ce86d5520c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16ecbddd6399447eb0369c1b612e1cc9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a55e6c8ee31441199c87b0df944334b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e29aed54f914bf5ac7401a8fe36fe11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"553572a7de0b432e9a2cea1f739c69c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b25dce548e0446b9e4d907be27d1e6a","IPY_MODEL_d77bf227770f42018bb96b206b13b61f","IPY_MODEL_d12cc6a687a049879c705f7d6481c5e9"],"layout":"IPY_MODEL_1fb0567e140740488d2cbcb5b7142208"}},"4b25dce548e0446b9e4d907be27d1e6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29500a6f214743beb2a8b3c338cf98dc","placeholder":"​","style":"IPY_MODEL_5ab1381799444f95b2ddd29b068895f1","value":"Running Prediction: 100%"}},"d77bf227770f42018bb96b206b13b61f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f236611c3af4518b2b510f0597045a5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3082598c61f4bdf8068113f74cb0e1a","value":1}},"d12cc6a687a049879c705f7d6481c5e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5af157987e944d3e8cf70183f0887917","placeholder":"​","style":"IPY_MODEL_3de9de05cc044fb2a860a0c6e64c42ec","value":" 1/1 [00:00&lt;00:00, 12.58it/s]"}},"1fb0567e140740488d2cbcb5b7142208":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29500a6f214743beb2a8b3c338cf98dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ab1381799444f95b2ddd29b068895f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f236611c3af4518b2b510f0597045a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3082598c61f4bdf8068113f74cb0e1a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5af157987e944d3e8cf70183f0887917":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3de9de05cc044fb2a860a0c6e64c42ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yusrashereen/qna-system-squad-v1?scriptVersionId=285097307\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 1. Setup and Installations","metadata":{"id":"xClLOnDVYlOu"}},{"cell_type":"code","source":"!pip install simpletransformers","metadata":{"id":"0HAu4ZE7YSm4","outputId":"0f7266a7-2a04-4328-b2b8-6b541dbba536","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:29:42.389584Z","iopub.execute_input":"2025-12-09T18:29:42.389846Z","iopub.status.idle":"2025-12-09T18:29:56.181064Z","shell.execute_reply.started":"2025-12-09T18:29:42.389827Z","shell.execute_reply":"2025-12-09T18:29:56.180412Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Imports and Logging Setup","metadata":{"id":"pA5mi12MYr7R"}},{"cell_type":"code","source":"import os\nimport json\nimport logging\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nfrom datetime import datetime","metadata":{"id":"f-XJvfa9YXed","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:29:56.182541Z","iopub.execute_input":"2025-12-09T18:29:56.18277Z","iopub.status.idle":"2025-12-09T18:30:29.532597Z","shell.execute_reply.started":"2025-12-09T18:29:56.182746Z","shell.execute_reply":"2025-12-09T18:30:29.531998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from simpletransformers.question_answering import QuestionAnsweringModel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T19:01:14.820999Z","iopub.execute_input":"2025-12-09T19:01:14.821295Z","iopub.status.idle":"2025-12-09T19:01:14.825205Z","shell.execute_reply.started":"2025-12-09T19:01:14.821266Z","shell.execute_reply":"2025-12-09T19:01:14.824323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"PyTorch version:\", torch.__version__)\nprint(\"Using CUDA:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"CUDA device:\", torch.cuda.get_device_name(0))","metadata":{"id":"FWE8fl7LYhWQ","outputId":"f2d7f657-ec5f-4444-8acf-6edffb3b650c","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:30:29.539995Z","iopub.execute_input":"2025-12-09T18:30:29.540299Z","iopub.status.idle":"2025-12-09T18:30:30.381269Z","shell.execute_reply.started":"2025-12-09T18:30:29.540278Z","shell.execute_reply":"2025-12-09T18:30:30.380458Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Download SQuAD 1.1 Dataset","metadata":{"id":"unmpfbAVYuvU"}},{"cell_type":"code","source":"os.makedirs(\"data\", exist_ok=True)\n\n# Download files\nprint(\"Downloading SQuAD v1.1 dataset...\")\n!wget -q -O data/train-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n!wget -q -O data/dev-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n\nprint(\"Files downloaded:\", os.listdir(\"data\"))\n","metadata":{"id":"9I0NabDFYvnf","outputId":"bc4448de-1892-42f6-8a28-68eb67b76314","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:30:30.382133Z","iopub.execute_input":"2025-12-09T18:30:30.382438Z","iopub.status.idle":"2025-12-09T18:30:31.563308Z","shell.execute_reply.started":"2025-12-09T18:30:30.382414Z","shell.execute_reply":"2025-12-09T18:30:31.562569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect dataset structure\nwith open(\"data/train-v1.1.json\", \"r\", encoding=\"utf-8\") as f:\n    squad_train_raw = json.load(f)\n\nprint(\"\\nDataset keys:\", squad_train_raw.keys())\nprint(\"Number of articles:\", len(squad_train_raw[\"data\"]))\nprint(\"Version:\", squad_train_raw[\"version\"])\n","metadata":{"id":"XvOGaE5iYzWq","outputId":"5a74df94-9c6f-49e8-bab1-380a7a83509e","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:30:31.564327Z","iopub.execute_input":"2025-12-09T18:30:31.564646Z","iopub.status.idle":"2025-12-09T18:30:32.361142Z","shell.execute_reply.started":"2025-12-09T18:30:31.564618Z","shell.execute_reply":"2025-12-09T18:30:32.360388Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Data Exploration and Statistics","metadata":{"id":"r73gvTPrZESo"}},{"cell_type":"code","source":"def explore_squad_data(file_path):\n  #Analyze SQuAD dataset statistics\n\n  with open(file_path, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n  stats = {\n  'total_articles': 0,\n  'total_paragraphs': 0,\n  'total_questions': 0,\n  'context_lengths': [],\n  'question_lengths': [],\n  'answer_lengths': [],\n  'answer_positions': []\n  }\n\n  for article in data[\"data\"]:\n    stats['total_articles'] += 1\n    for para in article[\"paragraphs\"]:\n      stats['total_paragraphs'] += 1\n      context_len = len(para[\"context\"].split())\n      stats['context_lengths'].append(context_len)\n\n      for qa in para[\"qas\"]:\n          stats['total_questions'] += 1\n          stats['question_lengths'].append(len(qa[\"question\"].split()))\n\n          if qa.get(\"answers\"):\n              answer = qa[\"answers\"][0]\n              stats['answer_lengths'].append(len(answer[\"text\"].split()))\n              stats['answer_positions'].append(answer[\"answer_start\"])\n\n  return stats","metadata":{"id":"oMFPIK0oZJA-","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:30:32.36194Z","iopub.execute_input":"2025-12-09T18:30:32.362256Z","iopub.status.idle":"2025-12-09T18:30:32.368908Z","shell.execute_reply.started":"2025-12-09T18:30:32.362236Z","shell.execute_reply":"2025-12-09T18:30:32.36803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"DATASET STATISTICS\")\nprint(\"=\"*60)\n\ntrain_stats = explore_squad_data(\"data/train-v1.1.json\")\ndev_stats = explore_squad_data(\"data/dev-v1.1.json\")\n\nprint(\"\\nTraining Set:\")\nprint(f\"  Articles: {train_stats['total_articles']}\")\nprint(f\"  Paragraphs: {train_stats['total_paragraphs']}\")\nprint(f\"  Questions: {train_stats['total_questions']}\")\nprint(f\"  Avg context length: {np.mean(train_stats['context_lengths']):.1f} words\")\nprint(f\"  Avg question length: {np.mean(train_stats['question_lengths']):.1f} words\")\nprint(f\"  Avg answer length: {np.mean(train_stats['answer_lengths']):.1f} words\")\n\nprint(\"\\nDevelopment Set:\")\nprint(f\"  Articles: {dev_stats['total_articles']}\")\nprint(f\"  Paragraphs: {dev_stats['total_paragraphs']}\")\nprint(f\"  Questions: {dev_stats['total_questions']}\")\nprint(f\"  Avg context length: {np.mean(dev_stats['context_lengths']):.1f} words\")\nprint(f\"  Avg question length: {np.mean(dev_stats['question_lengths']):.1f} words\")\nprint(f\"  Avg answer length: {np.mean(dev_stats['answer_lengths']):.1f} words\")","metadata":{"id":"CgeIOcVtZP-t","outputId":"dfce5f82-a489-4df9-fb16-036f2f688f72","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:30:32.36988Z","iopub.execute_input":"2025-12-09T18:30:32.370219Z","iopub.status.idle":"2025-12-09T18:30:33.473153Z","shell.execute_reply.started":"2025-12-09T18:30:32.370196Z","shell.execute_reply":"2025-12-09T18:30:33.472387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize distributions\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\naxes[0, 0].hist(train_stats['context_lengths'], bins=50, alpha=0.7, edgecolor='black')\naxes[0, 0].set_xlabel('Context Length (words)')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].set_title('Context Length Distribution')\naxes[0, 0].axvline(np.mean(train_stats['context_lengths']), color='red',\n                    linestyle='--', label=f'Mean: {np.mean(train_stats[\"context_lengths\"]):.1f}')\naxes[0, 0].legend()\n\naxes[0, 1].hist(train_stats['question_lengths'], bins=30, alpha=0.7,\n                color='green', edgecolor='black')\naxes[0, 1].set_xlabel('Question Length (words)')\naxes[0, 1].set_ylabel('Frequency')\naxes[0, 1].set_title('Question Length Distribution')\n\naxes[1, 0].hist(train_stats['answer_lengths'], bins=30, alpha=0.7,\n                color='orange', edgecolor='black')\naxes[1, 0].set_xlabel('Answer Length (words)')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].set_title('Answer Length Distribution')\naxes[1, 0].set_xlim(0, 20)\n\naxes[1, 1].hist(train_stats['answer_positions'], bins=50, alpha=0.7,\n                color='purple', edgecolor='black')\naxes[1, 1].set_xlabel('Answer Start Position (characters)')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].set_title('Answer Position Distribution')\n\nplt.tight_layout()\nplt.savefig('dataset_statistics.png', dpi=300, bbox_inches='tight')\nplt.show()\n","metadata":{"id":"1X5lpsTZZUyQ","outputId":"1c135764-cdce-41eb-87ca-5b6f0f309462","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:30:33.473919Z","iopub.execute_input":"2025-12-09T18:30:33.47413Z","iopub.status.idle":"2025-12-09T18:30:36.442155Z","shell.execute_reply.started":"2025-12-09T18:30:33.474115Z","shell.execute_reply":"2025-12-09T18:30:36.441519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Convert SQuAD to SimpleTransformers Format","metadata":{"id":"aO-2yqR_ZWwM"}},{"cell_type":"code","source":"def squad_to_simpletransformers(input_path):\n    \"\"\"Convert SQuAD v1.1 to SimpleTransformers format\"\"\"\n\n    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n        squad = json.load(f)\n\n    converted = []\n    skipped_count = 0\n\n    for article in squad[\"data\"]:\n        for para in article[\"paragraphs\"]:\n            context = para[\"context\"]\n            qas_list = []\n\n            for qa in para[\"qas\"]:\n                answers = qa.get(\"answers\", [])\n\n                if not answers:\n                    skipped_count += 1\n                    continue\n\n                qas_entry = {\n                    \"id\": qa[\"id\"],\n                    \"is_impossible\": False,\n                    \"question\": qa[\"question\"],\n                    \"answers\": answers,\n                }\n                qas_list.append(qas_entry)\n\n            if qas_list:\n                converted.append({\"context\": context, \"qas\": qas_list})\n\n    if skipped_count > 0:\n        print(f\"Skipped {skipped_count} questions without answers\")\n\n    return converted\n\nprint(\"\\nConverting datasets...\")\ntrain_data = squad_to_simpletransformers(\"data/train-v1.1.json\")\ndev_data = squad_to_simpletransformers(\"data/dev-v1.1.json\")\n\nprint(f\"Training contexts: {len(train_data)}\")\nprint(f\"Development contexts: {len(dev_data)}\")\n","metadata":{"id":"YdgcJ5MbZeBL","outputId":"7046235d-10fb-4d31-a553-ac0816d55189","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:30:36.444762Z","iopub.execute_input":"2025-12-09T18:30:36.445319Z","iopub.status.idle":"2025-12-09T18:30:37.318213Z","shell.execute_reply.started":"2025-12-09T18:30:36.445297Z","shell.execute_reply":"2025-12-09T18:30:37.317422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save converted data\nwith open(\"data/train_simple.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(train_data, f)\n\nwith open(\"data/dev_simple.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(dev_data, f)\n\nprint(\"\\nSample converted entry:\")\nprint(json.dumps(train_data[0], indent=2)[:500] + \"...\")\n","metadata":{"id":"s2EFOMJmZgSp","outputId":"d06d6f07-2199-4a9d-b5aa-d3eeb03ecbb3","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:30:37.319052Z","iopub.execute_input":"2025-12-09T18:30:37.319352Z","iopub.status.idle":"2025-12-09T18:30:38.586595Z","shell.execute_reply.started":"2025-12-09T18:30:37.319329Z","shell.execute_reply":"2025-12-09T18:30:38.58587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\nhalf = len(train_data) // 2\nsmall_train = random.sample(train_data, half)\n\nwith open(\"data/train_simple.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(small_train, f, ensure_ascii=False, indent=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Evaluation Metrics Implementation","metadata":{"id":"Xnh8YaChZiod"}},{"cell_type":"code","source":"def normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    import re\n    import string\n    \n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    \n    def white_space_fix(text):\n        return ' '.join(text.split())\n    \n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    \n    def lower(text):\n        return text.lower()\n    \n    return white_space_fix(remove_articles(remove_punc(lower(s))))","metadata":{"id":"JHTFKynUZn8F","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T19:49:14.371389Z","iopub.execute_input":"2025-12-09T19:49:14.371939Z","iopub.status.idle":"2025-12-09T19:49:14.37708Z","shell.execute_reply.started":"2025-12-09T19:49:14.371915Z","shell.execute_reply":"2025-12-09T19:49:14.37648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef exact_match_score(prediction, ground_truth):\n    \"\"\"Compute exact match score.\"\"\"\n    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n\ndef f1_score(prediction, ground_truth):\n    \"\"\"Compute F1 score.\"\"\"\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    \n    common = set(prediction_tokens) & set(ground_truth_tokens)\n    num_same = len(common)\n    \n    if num_same == 0:\n        return 0\n    \n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    \n    return f1","metadata":{"id":"jHU8bymRZtn3","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:52:08.091577Z","iopub.execute_input":"2025-12-09T18:52:08.091805Z","iopub.status.idle":"2025-12-09T18:52:08.096901Z","shell.execute_reply.started":"2025-12-09T18:52:08.091788Z","shell.execute_reply":"2025-12-09T18:52:08.096125Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_question_lookup(eval_data):\n    # lookup dictionary mapping question IDs to their context, question, and answers.\n    \n    q_id_to_qa = {}\n    for article in eval_data:\n        context = article['context']\n        for qa in article['qas']:\n            q_id_to_qa[qa['id']] = {\n                'context': context,\n                'question': qa['question'],\n                'true_answers': [ans['text'] for ans in qa['answers']]\n            }\n    return q_id_to_qa     # dictionary mapping question IDs to question data","metadata":{"id":"ORKgoCQSZwEN","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:52:15.336301Z","iopub.execute_input":"2025-12-09T18:52:15.336594Z","iopub.status.idle":"2025-12-09T18:52:15.341391Z","shell.execute_reply.started":"2025-12-09T18:52:15.336573Z","shell.execute_reply":"2025-12-09T18:52:15.340627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef normalize_predictions(model_predictions):\n    \n    predictions_list = []\n    \n    # Check if predictions are in the categorized format\n    if isinstance(model_predictions, dict):\n        # Check for the three category keys\n        has_categories = all(k in model_predictions for k in ['correct_text', 'similar_text', 'incorrect_text'])\n        \n        if has_categories:\n            logger.info(\"Detected categorized prediction format (correct_text, similar_text, incorrect_text)\")\n            \n            # Process correct predictions (simple string format)\n            if 'correct_text' in model_predictions:\n                correct_dict = model_predictions['correct_text']\n                logger.info(f\"Processing {len(correct_dict)} correct predictions\")\n                \n                for q_id, answer in correct_dict.items():\n                    predictions_list.append({\n                        'id': q_id,\n                        'answer': answer,\n                        'probability': None,  # Not provided for correct answers\n                        'category': 'correct'\n                    })\n            \n            # Process similar predictions (dict format with truth/predicted)\n            if 'similar_text' in model_predictions:\n                similar_dict = model_predictions['similar_text']\n                logger.info(f\"Processing {len(similar_dict)} similar predictions\")\n                \n                for q_id, pred_data in similar_dict.items():\n                    if isinstance(pred_data, dict):\n                        predictions_list.append({\n                            'id': q_id,\n                            'answer': pred_data.get('predicted', ''),\n                            'probability': None,\n                            'category': 'similar',\n                            'truth': pred_data.get('truth', ''),\n                            'question': pred_data.get('question', '')\n                        })\n                    else:\n                        # Fallback if format is different\n                        predictions_list.append({\n                            'id': q_id,\n                            'answer': pred_data,\n                            'probability': None,\n                            'category': 'similar'\n                        })\n            \n            # Process incorrect predictions (dict format with truth/predicted)\n            if 'incorrect_text' in model_predictions:\n                incorrect_dict = model_predictions['incorrect_text']\n                logger.info(f\"Processing {len(incorrect_dict)} incorrect predictions\")\n                \n                for q_id, pred_data in incorrect_dict.items():\n                    if isinstance(pred_data, dict):\n                        predictions_list.append({\n                            'id': q_id,\n                            'answer': pred_data.get('predicted', ''),\n                            'probability': None,\n                            'category': 'incorrect',\n                            'truth': pred_data.get('truth', ''),\n                            'question': pred_data.get('question', '')\n                        })\n                    else:\n                        # Fallback if format is different\n                        predictions_list.append({\n                            'id': q_id,\n                            'answer': pred_data,\n                            'probability': None,\n                            'category': 'incorrect'\n                        })\n            \n            logger.info(f\"Total predictions after normalization: {len(predictions_list)}\")\n            return predictions_list\n        \n        else:\n            # Old format: flat dictionary with question IDs as keys\n            logger.info(\"Detected flat dictionary format\")\n            for k, v in model_predictions.items():\n                if isinstance(v, dict):\n                    pred = {\"id\": k, **v}\n                else:\n                    pred = {\"id\": k, \"answer\": v}\n                predictions_list.append(pred)\n            return predictions_list\n    \n    else:\n        # Already a list\n        logger.info(f\"Predictions already in list format with {len(model_predictions)} items\")\n        return model_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T19:50:25.522788Z","iopub.execute_input":"2025-12-09T19:50:25.523062Z","iopub.status.idle":"2025-12-09T19:50:25.528754Z","shell.execute_reply.started":"2025-12-09T19:50:25.523042Z","shell.execute_reply":"2025-12-09T19:50:25.527915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef analyze_predictions(model_predictions, q_id_to_qa):\n    \n    # Normalize predictions to list format\n    predictions_list = normalize_predictions(model_predictions)\n    \n    logger.info(f\"Processing {len(predictions_list)} predictions after normalization\")\n    \n    misclassified_examples = []\n    correct_examples = []\n    skipped_count = 0\n    \n    for qa_pred in predictions_list:\n        q_id = qa_pred.get(\"id\")\n        \n        if q_id is None:\n            logger.warning(f\"Skipping prediction with missing ID\")\n            skipped_count += 1\n            continue\n            \n        if q_id not in q_id_to_qa:\n            logger.debug(f\"Prediction ID not found in original data: {q_id}\")\n            skipped_count += 1\n            continue\n        \n        # Extract prediction\n        pred_answer = qa_pred.get(\"answer\", \"\")\n        if isinstance(pred_answer, list):\n            pred_answer = pred_answer[0] if pred_answer else \"\"\n        \n        # Get probability if available\n        probability = qa_pred.get(\"probability\")\n        if probability and isinstance(probability, list):\n            probability = probability[0]\n        \n        # Get category if available\n        category = qa_pred.get(\"category\", \"unknown\")\n        \n        # Get true data\n        qa_true = q_id_to_qa[q_id]\n        true_answers = qa_true[\"true_answers\"]\n        question = qa_true[\"question\"]\n        context = qa_true[\"context\"]\n        \n        # Compute metrics across all true answers\n        em_scores = [exact_match_score(pred_answer, t) for t in true_answers]\n        f1_scores = [f1_score(pred_answer, t) for t in true_answers]\n        \n        em = max(em_scores) if em_scores else 0\n        f1 = max(f1_scores) if f1_scores else 0\n        \n        # Create example record\n        example = {\n            \"question_id\": q_id,\n            \"context\": context,\n            \"question\": question,\n            \"predicted_answer\": pred_answer,\n            \"true_answers\": true_answers,\n            \"probability\": probability,\n            \"exact_match\": em,\n            \"f1_score\": f1,\n            \"category\": category\n        }\n        \n        # Add truth if available (for similar/incorrect categories)\n        if \"truth\" in qa_pred:\n            example[\"expected_truth\"] = qa_pred[\"truth\"]\n        \n        # For misclassifications, focus on 'incorrect_text' category\n        # This is more efficient and accurate than computing EM for everything\n        if category == \"incorrect\":\n            misclassified_examples.append(example)\n        elif category == \"similar\":\n            # Similar means partial match (some F1 score but EM=0)\n            # Include these as misclassified since EM=0\n            misclassified_examples.append(example)\n        elif category == \"correct\":\n            correct_examples.append(example)\n        else:\n            # Fallback: compute EM if category is unknown\n            if em == 0:\n                misclassified_examples.append(example)\n            else:\n                correct_examples.append(example)\n    \n    if skipped_count > 0:\n        logger.info(f\"Skipped {skipped_count} predictions (not found in eval data)\")\n    \n    logger.info(f\"Found {len(misclassified_examples)} misclassified (incorrect + similar)\")\n    logger.info(f\"  - From 'incorrect' category: {sum(1 for e in misclassified_examples if e['category'] == 'incorrect')}\")\n    logger.info(f\"  - From 'similar' category: {sum(1 for e in misclassified_examples if e['category'] == 'similar')}\")\n    logger.info(f\"Found {len(correct_examples)} correct (exact matches)\")\n    \n    return misclassified_examples, correct_examples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:37:05.129025Z","iopub.execute_input":"2025-12-10T05:37:05.129281Z","iopub.status.idle":"2025-12-10T05:37:05.141019Z","shell.execute_reply.started":"2025-12-10T05:37:05.129262Z","shell.execute_reply":"2025-12-10T05:37:05.140303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef save_misclassified_examples(misclassified_examples, output_file, metadata=None):\n    # save misclassified examples to a JSON file.\n   \n    output_data = {\n        \"metadata\": metadata or {},\n        \"misclassified_examples\": misclassified_examples\n    }\n    \n    # Create output directory if needed\n    output_dir = os.path.dirname(output_file)\n    if output_dir:\n        os.makedirs(output_dir, exist_ok=True)\n    \n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(output_data, f, indent=2, ensure_ascii=False)\n    \n    logger.info(f\"Saved {len(misclassified_examples)} misclassified examples to {output_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:54:40.813352Z","iopub.execute_input":"2025-12-09T18:54:40.813657Z","iopub.status.idle":"2025-12-09T18:54:40.818766Z","shell.execute_reply.started":"2025-12-09T18:54:40.813638Z","shell.execute_reply":"2025-12-09T18:54:40.817978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef print_sample_examples(examples, num_samples=5, title=\"PREDICTIONS\"):\n    \n    print(\"\\n\" + \"=\"*70)\n    print(title)\n    print(\"=\"*70)\n    \n    for i, example in enumerate(examples[:num_samples], 1):\n        print(f\"\\n--- Example {i} ---\")\n        print(f\"Question ID: {example['question_id']}\")\n        print(f\"Question: {example['question']}\")\n        print(f\"Context (first 200 chars): {example['context'][:200]}...\")\n        print(f\"Predicted Answer: {example['predicted_answer']}\")\n        print(f\"True Answers: {example['true_answers']}\")\n        if example.get('probability'):\n            print(f\"Probability: {example['probability']:.3f}\")\n        print(f\"Exact Match: {example['exact_match']}\")\n        print(f\"F1 Score: {example['f1_score']:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:55:46.010245Z","iopub.execute_input":"2025-12-09T18:55:46.011148Z","iopub.status.idle":"2025-12-09T18:55:46.016026Z","shell.execute_reply.started":"2025-12-09T18:55:46.01112Z","shell.execute_reply":"2025-12-09T18:55:46.015257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_em_f1(predictions, references):\n\n    \"\"\"Compute EM and F1 for a set of predictions\"\"\"\n\n    em_scores = []\n    f1_scores = []\n\n    for pred, ref in zip(predictions, references):\n\n        # Handle multiple reference answers (take max score)\n\n        if isinstance(ref, list):\n            em = max(exact_match_score(pred, r) for r in ref)\n            f1 = max(f1_score(pred, r) for r in ref)\n        else:\n            em = exact_match_score(pred, ref)\n            f1 = f1_score(pred, ref)\n\n        em_scores.append(em)\n        f1_scores.append(f1)\n\n    return {\n        'exact_match': np.mean(em_scores) * 100,\n        'f1': np.mean(f1_scores) * 100,\n        'em_scores': em_scores,\n        'f1_scores': f1_scores\n    }\n","metadata":{"id":"6TWN9keGZxww","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:30:38.712734Z","iopub.execute_input":"2025-12-09T18:30:38.712963Z","iopub.status.idle":"2025-12-09T18:30:38.72704Z","shell.execute_reply.started":"2025-12-09T18:30:38.712944Z","shell.execute_reply":"2025-12-09T18:30:38.726421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def perform_error_analysis(model_predictions_flattened, original_eval_data_squad_format, num_examples_to_analyze=None):\n\n    # Build lookup for true data by question ID\n    q_id_to_qa_true = {}\n    for article in original_eval_data_squad_format:\n        context = article['context']\n        for qa in article['qas']:\n            q_id_to_qa_true[qa['id']] = {\n                'context': context,\n                'question': qa['question'],\n                'true_answers': [ans['text'] for ans in qa['answers']]\n            }\n\n    error_cases = []\n    correct_cases = []\n\n    # Always treat predictions as a list (same as inference)\n    predictions_to_process = model_predictions_flattened\n\n    if num_examples_to_analyze:\n        predictions_to_process = predictions_to_process[:num_examples_to_analyze]\n\n    # Iterate through each prediction\n    for qa_pred in predictions_to_process:\n\n        q_id = qa_pred.get(\"id\")\n        if q_id is None:\n            logging.warning(f\"Skipping prediction with missing ID: {qa_pred}\")\n            continue\n\n        if q_id not in q_id_to_qa_true:\n            logging.warning(f\"Prediction ID not found in original data: {q_id}\")\n            continue\n\n        # Extract prediction fields\n        pred_answer = qa_pred[\"answer\"][0] if isinstance(qa_pred[\"answer\"], list) else qa_pred[\"answer\"]\n        probability = qa_pred[\"probability\"][0] if (\"probability\" in qa_pred and qa_pred[\"probability\"]) else None\n\n        qa_true = q_id_to_qa_true[q_id]\n        true_answers = qa_true[\"true_answers\"]\n        question = qa_true[\"question\"]\n        context = qa_true[\"context\"]\n\n        # Compute EM/F1 across multiple true answers\n        em = max(exact_match_score(pred_answer, t) for t in true_answers)\n        f1 = max(f1_score(pred_answer, t) for t in true_answers)\n\n        # Build record\n        case = {\n            \"question_id\": q_id,\n            \"context\": context,\n            \"question\": question,\n            \"predicted_answer\": pred_answer,\n            \"true_answers\": true_answers,\n            \"probability\": probability,\n            \"exact_match\": em,\n            \"f1_score\": f1,\n        }\n\n        if em == 0:\n            error_cases.append(case)\n        else:\n            correct_cases.append(case)\n\n    return error_cases, correct_cases\n","metadata":{"id":"J9A2q1bM_bHg","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:30:38.727988Z","iopub.execute_input":"2025-12-09T18:30:38.728327Z","iopub.status.idle":"2025-12-09T18:30:38.741975Z","shell.execute_reply.started":"2025-12-09T18:30:38.728305Z","shell.execute_reply":"2025-12-09T18:30:38.741361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Model Training Function","metadata":{"id":"RAL0M91rZ0e0"}},{"cell_type":"code","source":"\ndef train_and_evaluate_model(\n    model_type,\n    model_name,\n    train_file,\n    eval_file,\n    model_args,\n    experiment_name=None\n):\n    \n    if experiment_name is None:\n        experiment_name = f\"{model_type}_{model_name}\"\n    \n    print(\"\\n\" + \"=\"*70)\n    print(f\"EXPERIMENT: {experiment_name}\")\n    print(f\"Model: {model_type}/{model_name}\")\n    print(\"=\"*70)\n    \n    # Create model\n    logger.info(f\"Initializing {model_type}/{model_name}...\")\n    qa_model = QuestionAnsweringModel(\n        model_type,\n        model_name,\n        args=model_args,\n        use_cuda=torch.cuda.is_available(),\n    )\n    \n    # Training\n    start_time = datetime.now()\n    logger.info(f\"Training started at {start_time.strftime('%H:%M:%S')}\")\n    qa_model.train_model(train_file)\n    training_time = (datetime.now() - start_time).total_seconds()\n    logger.info(f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n    \n    # Evaluation\n    logger.info(\"Evaluating on development set...\")\n    eval_results, predictions = qa_model.eval_model(eval_file)\n    \n    print(\"\\nEvaluation Metrics:\")\n    print(json.dumps(eval_results, indent=2))\n    \n    # Debug: Inspect predictions structure\n    logger.info(\"\\n\" + \"=\"*70)\n    logger.info(\"PREDICTION STRUCTURE DEBUG\")\n    logger.info(\"=\"*70)\n    logger.info(f\"Type of predictions: {type(predictions)}\")\n    \n    if isinstance(predictions, dict):\n        logger.info(f\"Number of keys in predictions dict: {len(predictions)}\")\n        logger.info(f\"Keys in predictions: {list(predictions.keys())[:10]}\")\n        \n        # Sample a non-metadata key if available\n        non_metadata_keys = [k for k in predictions.keys() if k not in \n                            {'correct_text', 'similar_text', 'incorrect_text', 'correct', 'similar', 'incorrect'}]\n        \n        if non_metadata_keys:\n            sample_key = non_metadata_keys[0]\n            logger.info(f\"\\nSample prediction (key={sample_key}):\")\n            logger.info(f\"  Value: {predictions[sample_key]}\")\n            logger.info(f\"  Value type: {type(predictions[sample_key])}\")\n    elif isinstance(predictions, list):\n        logger.info(f\"Number of predictions in list: {len(predictions)}\")\n        if predictions:\n            logger.info(f\"Sample prediction: {predictions[0]}\")\n    else:\n        logger.warning(f\"Unexpected predictions type: {type(predictions)}\")\n    \n    logger.info(\"=\"*70 + \"\\n\")\n    \n    # Clean up GPU memory\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    return qa_model, eval_results, predictions, training_time\n","metadata":{"id":"ynFNMpDhZzi_","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:57:01.82684Z","iopub.execute_input":"2025-12-09T18:57:01.827783Z","iopub.status.idle":"2025-12-09T18:57:01.833936Z","shell.execute_reply.started":"2025-12-09T18:57:01.827757Z","shell.execute_reply":"2025-12-09T18:57:01.833089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_and_save_errors(\n    predictions,\n    eval_file,\n    output_file,\n    model_type=None,\n    model_name=None,\n    experiment_name=None,\n    min_examples=5\n):\n\n#    Focuses on 'incorrect' category only (true errors where EM=0 and F1≈0).\n#    Excludes 'similar' category (partial matches) for cleaner error analysis.\n \n    # Load original eval data\n    logger.info(\"Loading evaluation data...\")\n    with open(eval_file, \"r\", encoding=\"utf-8\") as f:\n        eval_data = json.load(f)\n    \n    # Build question lookup\n    logger.info(\"Building question lookup...\")\n    q_id_to_qa = build_question_lookup(eval_data)\n    \n    # Analyze predictions\n    logger.info(\"Analyzing predictions...\")\n    misclassified, correct = analyze_predictions(predictions, q_id_to_qa)\n    \n    # Filter to only 'incorrect' category (true errors)\n    # Exclude 'similar' category (partial matches) for cleaner error analysis\n    original_count = len(misclassified)\n    misclassified = [e for e in misclassified if e['category'] == 'incorrect']\n    similar_count = original_count - len(misclassified)\n    \n    logger.info(f\"Found {len(misclassified)} true misclassifications (incorrect category)\")\n    logger.info(f\"Excluded {similar_count} partial matches (similar category)\")\n    logger.info(f\"Found {len(correct)} correct predictions (exact matches)\")\n    \n    if len(misclassified) < min_examples:\n        logger.warning(\n            f\"Only found {len(misclassified)} misclassified examples \"\n            f\"(minimum requested: {min_examples})\"\n        )\n    \n    # Prepare metadata\n    metadata = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"eval_file\": eval_file,\n        \"total_predictions\": len(misclassified) + similar_count + len(correct),\n        \"misclassified_count\": len(misclassified),\n        \"similar_count\": similar_count,\n        \"correct_count\": len(correct),\n        \"analysis_note\": \"Only 'incorrect' category included (true errors). 'Similar' category (partial matches) excluded.\",\n    }\n    \n    if model_type:\n        metadata[\"model_type\"] = model_type\n    if model_name:\n        metadata[\"model_name\"] = model_name\n    if experiment_name:\n        metadata[\"experiment_name\"] = experiment_name\n    \n    # Save misclassified examples\n    save_misclassified_examples(misclassified, output_file, metadata)\n    \n    # Print samples\n    if misclassified:\n        print_sample_examples(\n            misclassified, \n            num_samples=min(5, len(misclassified)),\n            title=\"MISCLASSIFIED EXAMPLES\"\n        )\n    \n    return misclassified, correct","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:58:26.326445Z","iopub.execute_input":"2025-12-09T18:58:26.326913Z","iopub.status.idle":"2025-12-09T18:58:26.333489Z","shell.execute_reply.started":"2025-12-09T18:58:26.326891Z","shell.execute_reply":"2025-12-09T18:58:26.332688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pipeline(\n    model_type,\n    model_name,\n    train_file,\n    eval_file,\n    model_args,\n    output_file=None,\n    experiment_name=None,\n    min_examples=5\n):\n    if experiment_name is None:\n        experiment_name = f\"{model_type}_{model_name.replace('/', '_')}\"\n    \n    if output_file is None:\n        output_file = f\"outputs/{experiment_name}_misclassified.json\"\n    \n    # Train and evaluate\n    model, eval_results, predictions, training_time = train_and_evaluate_model(\n        model_type=model_type,\n        model_name=model_name,\n        train_file=train_file,\n        eval_file=eval_file,\n        model_args=model_args,\n        experiment_name=experiment_name\n    )\n    \n    # Extract errors\n    misclassified, correct = extract_and_save_errors(\n        predictions=predictions,\n        eval_file=eval_file,\n        output_file=output_file,\n        model_type=model_type,\n        model_name=model_name,\n        experiment_name=experiment_name,\n        min_examples=min_examples\n    )\n    \n    # Return all results\n    return {\n        \"model\": model,\n        \"eval_results\": eval_results,\n        \"predictions\": predictions,\n        \"training_time\": training_time,\n        \"misclassified_examples\": misclassified,\n        \"correct_examples\": correct,\n        \"output_file\": output_file\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:59:53.102728Z","iopub.execute_input":"2025-12-09T18:59:53.103011Z","iopub.status.idle":"2025-12-09T18:59:53.108544Z","shell.execute_reply.started":"2025-12-09T18:59:53.102991Z","shell.execute_reply":"2025-12-09T18:59:53.107896Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. BERT-base-uncased (Baseline)","metadata":{"id":"YxGc_9QMZ-g-"}},{"cell_type":"code","source":"bert_args = {\n    \"learning_rate\": 3e-5,\n    \"num_train_epochs\": 1,\n    \"max_seq_length\": 384,\n    \"doc_stride\": 128,\n    \"train_batch_size\": 8,\n    \"eval_batch_size\": 8,\n    \"n_best_size\": 20,\n    \"max_answer_length\": 30,\n    \"overwrite_output_dir\": True,\n    \"reprocess_input_data\": True,\n    \"do_lower_case\": True,\n    \"output_dir\": \"outputs/bert-squad\",\n    \"best_model_dir\": \"outputs/bert-squad/best_model\",\n    \"save_steps\": 5000,\n    \"logging_steps\": 500,\n    \"fp16\": False,\n}","metadata":{"id":"MKf8tFJGZ8qn","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:30:38.756791Z","iopub.execute_input":"2025-12-09T18:30:38.757066Z","iopub.status.idle":"2025-12-09T18:30:38.773979Z","shell.execute_reply.started":"2025-12-09T18:30:38.757044Z","shell.execute_reply":"2025-12-09T18:30:38.773204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bert_results = pipeline(\n        model_type=\"bert\",\n        model_name=\"bert-base-uncased\",\n        train_file=\"data/train_simple.json\",\n        eval_file=\"data/dev_simple.json\",\n        model_args=bert_args,\n        experiment_name=\"BERT_Base_Baseline\",\n        min_examples=5\n    )","metadata":{"id":"hIap_-vTbBGX","outputId":"3890107b-d016-42d0-92c4-771c3946406f","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T19:51:33.654396Z","iopub.execute_input":"2025-12-09T19:51:33.655065Z","iopub.status.idle":"2025-12-09T20:05:08.607007Z","shell.execute_reply.started":"2025-12-09T19:51:33.655034Z","shell.execute_reply":"2025-12-09T20:05:08.60625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_context = (\n    \"Natural Language Processing (NLP) is a field of Artificial Intelligence that focuses \"\n    \"on enabling computers to understand and generate human language.\"\n)\n\nto_predict = [\n    {\n        \"context\": custom_context,\n        \"qas\": [\n            {\n                \"id\": \"demo-1\",\n                \"question\": \"What does NLP focus on?\",\n            },\n            {\n                \"id\": \"demo-2\",\n                \"question\": \"NLP is a subfield of what broader area?\",\n            },\n        ],\n    }\n]\n\npredictions, raw_outputs = bert_results['model'].predict(to_predict)\n\n# Create mappings for easy lookup\nprediction_map = {pred['id']: pred for pred in predictions}\nraw_output_map = {output['id']: output for output in raw_outputs}\n\n# Iterate through the original input structure to get the question text\nfor context_qa_block in to_predict:\n    for qa_input in context_qa_block[\"qas\"]:\n        qa_id = qa_input[\"id\"]\n        question = qa_input[\"question\"]\n\n        # Get the corresponding prediction and raw output\n        if qa_id in prediction_map:\n            prediction = prediction_map[qa_id]\n            # Take the first (most confident) answer from the list\n            answer = prediction[\"answer\"][0]\n\n            # Retrieve probability from raw_output_map\n            raw_output_entry = raw_output_map.get(qa_id)\n            probability = raw_output_entry['probability'][0] if raw_output_entry and 'probability' in raw_output_entry and raw_output_entry['probability'] else None\n\n            print(f\"Q ({qa_id}): {question}\")\n            # Print question, answer and score\n            if probability is not None:\n                print(f\"  -> Answer: {answer!r}, score={probability:.4f}\")\n            else:\n                print(f\"  -> Answer: {answer!r}, score=N/A\")\n        else:\n            print(f\"Q ({qa_id}): {question}\")\n            print(f\"  -> No prediction found for this question.\")\n\n","metadata":{"id":"_WMAs7Ef_7_R","outputId":"336b5f1c-6e7e-4627-bc85-f266029c5d89","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.003877Z","iopub.status.idle":"2025-12-09T18:44:10.004208Z","shell.execute_reply.started":"2025-12-09T18:44:10.004058Z","shell.execute_reply":"2025-12-09T18:44:10.004072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"oYc0A_H1AnEu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9. ALBERT-base-v2","metadata":{"id":"EhtoJT88bGLj"}},{"cell_type":"code","source":"albert_args = bert_args.copy()\nalbert_args[\"output_dir\"] = \"outputs/albert-squad\"\nalbert_args[\"best_model_dir\"] = \"outputs/albert-squad/best_model\"","metadata":{"id":"sOm8UI-2bC30","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.005639Z","iopub.status.idle":"2025-12-09T18:44:10.005916Z","shell.execute_reply.started":"2025-12-09T18:44:10.005767Z","shell.execute_reply":"2025-12-09T18:44:10.005783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"albert_results = pipeline(\n    model_type=\"albert\",\n    model_name=\"albert-base-v2\",\n    experiment_name=\"ALBERT-base-v2\",\n    train_file=\"data/train_simple.json\",\n    eval_file=\"data/dev_simple.json\",\n    model_args=albert_args,\n    min_examples=5\n)","metadata":{"id":"-6V5fKGkbMHg","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.007157Z","iopub.status.idle":"2025-12-09T18:44:10.007405Z","shell.execute_reply.started":"2025-12-09T18:44:10.007269Z","shell.execute_reply":"2025-12-09T18:44:10.007278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_context = (\n    \"Natural Language Processing (NLP) is a field of Artificial Intelligence that focuses \"\n    \"on enabling computers to understand and generate human language.\"\n)\n\nto_predict = [\n    {\n        \"context\": custom_context,\n        \"qas\": [\n            {\n                \"id\": \"demo-1\",\n                \"question\": \"What does NLP focus on?\",\n            },\n            {\n                \"id\": \"demo-2\",\n                \"question\": \"NLP is a subfield of what broader area?\",\n            },\n        ],\n    }\n]\n\npredictions, raw_outputs = albert_results['model'].predict(to_predict)\n\n# Create mappings for easy lookup\nprediction_map = {pred['id']: pred for pred in predictions}\nraw_output_map = {output['id']: output for output in raw_outputs}\n\n# Iterate through the original input structure to get the question text\nfor context_qa_block in to_predict:\n    for qa_input in context_qa_block[\"qas\"]:\n        qa_id = qa_input[\"id\"]\n        question = qa_input[\"question\"]\n\n        # Get the corresponding prediction and raw output\n        if qa_id in prediction_map:\n            prediction = prediction_map[qa_id]\n            # Take the first (most confident) answer from the list\n            answer = prediction[\"answer\"][0]\n\n            # Retrieve probability from raw_output_map\n            raw_output_entry = raw_output_map.get(qa_id)\n            probability = raw_output_entry['probability'][0] if raw_output_entry and 'probability' in raw_output_entry and raw_output_entry['probability'] else None\n\n            print(f\"Q ({qa_id}): {question}\")\n            # Print question, answer and score\n            if probability is not None:\n                print(f\"  -> Answer: {answer!r}, score={probability:.4f}\")\n            else:\n                print(f\"  -> Answer: {answer!r}, score=N/A\")\n        else:\n            print(f\"Q ({qa_id}): {question}\")\n            print(f\"  -> No prediction found for this question.\")\n\n","metadata":{"id":"mS6ZNmQOB73i","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.00814Z","iopub.status.idle":"2025-12-09T18:44:10.008362Z","shell.execute_reply.started":"2025-12-09T18:44:10.008251Z","shell.execute_reply":"2025-12-09T18:44:10.008261Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10. MobileBERT","metadata":{"id":"n1oYAJyUbTSe"}},{"cell_type":"code","source":"mobilebert_args = bert_args.copy()\nmobilebert_args[\"output_dir\"] = \"outputs/mobilebert-squad\"\nmobilebert_args[\"best_model_dir\"] = \"outputs/mobilebert-squad/best_model\"","metadata":{"id":"eNDRrcfYbV5B","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.009472Z","iopub.status.idle":"2025-12-09T18:44:10.009798Z","shell.execute_reply.started":"2025-12-09T18:44:10.009627Z","shell.execute_reply":"2025-12-09T18:44:10.009641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mobilebert_results= pipeline(\n    model_type=\"mobilebert\",\n    model_name=\"google/mobilebert-uncased\",\n    train_file=\"data/train_simple.json\",\n    eval_file=\"data/dev_simple.json\",\n    experiment_name=\"MobileBERT\",\n    model_args=mobilebert_args,\n    min_examples=5\n)","metadata":{"id":"xJKf7cJhbY3e","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.012243Z","iopub.status.idle":"2025-12-09T18:44:10.012518Z","shell.execute_reply.started":"2025-12-09T18:44:10.012392Z","shell.execute_reply":"2025-12-09T18:44:10.012406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_context = (\n    \"Natural Language Processing (NLP) is a field of Artificial Intelligence that focuses \"\n    \"on enabling computers to understand and generate human language.\"\n)\n\nto_predict = [\n    {\n        \"context\": custom_context,\n        \"qas\": [\n            {\n                \"id\": \"demo-1\",\n                \"question\": \"What does NLP focus on?\",\n            },\n            {\n                \"id\": \"demo-2\",\n                \"question\": \"NLP is a subfield of what broader area?\",\n            },\n        ],\n    }\n]\n\npredictions, raw_outputs = mobilebert_results['model'].predict(to_predict)\n\n# Create mappings for easy lookup\nprediction_map = {pred['id']: pred for pred in predictions}\nraw_output_map = {output['id']: output for output in raw_outputs}\n\n# Iterate through the original input structure to get the question text\nfor context_qa_block in to_predict:\n    for qa_input in context_qa_block[\"qas\"]:\n        qa_id = qa_input[\"id\"]\n        question = qa_input[\"question\"]\n\n        # Get the corresponding prediction and raw output\n        if qa_id in prediction_map:\n            prediction = prediction_map[qa_id]\n            # Take the first (most confident) answer from the list\n            answer = prediction[\"answer\"][0]\n\n            # Retrieve probability from raw_output_map\n            raw_output_entry = raw_output_map.get(qa_id)\n            probability = raw_output_entry['probability'][0] if raw_output_entry and 'probability' in raw_output_entry and raw_output_entry['probability'] else None\n\n            print(f\"Q ({qa_id}): {question}\")\n            # Print question, answer and score\n            if probability is not None:\n                print(f\"  -> Answer: {answer!r}, score={probability:.4f}\")\n            else:\n                print(f\"  -> Answer: {answer!r}, score=N/A\")\n        else:\n            print(f\"Q ({qa_id}): {question}\")\n            print(f\"  -> No prediction found for this question.\")\n\n","metadata":{"id":"vFYvTix3CBP3","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.013801Z","iopub.status.idle":"2025-12-09T18:44:10.014027Z","shell.execute_reply.started":"2025-12-09T18:44:10.013923Z","shell.execute_reply":"2025-12-09T18:44:10.013932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 11. Hyperparameter Tuning\n## No. of Epochs & Learning Rate","metadata":{"id":"3KAX6UmJba7r"}},{"cell_type":"code","source":"mobilebert_tuned_args = bert_args.copy()\nmobilebert_tuned_args[\"num_train_epochs\"] = 5 # initially trained for 1 epoch\nmobilebert_tuned_args[\"learning_rate\"] = 5e-5 # initially 3e-5\nmobilebert_tuned_args[\"output_dir\"] = \"outputs/mobilebert-squad-tuned\"\nmobilebert_tuned_args[\"best_model_dir\"] = \"outputs/mobilebert-squad-tuned/best_model\"\n","metadata":{"id":"9UUBFnGrbfHb","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.014698Z","iopub.status.idle":"2025-12-09T18:44:10.014911Z","shell.execute_reply.started":"2025-12-09T18:44:10.014809Z","shell.execute_reply":"2025-12-09T18:44:10.014818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mobilebert_tuned_results= pipeline(\n    model_type=\"mobilebert\",\n    model_name=\"google/mobilebert-uncased\",\n    train_file=\"data/train_simple.json\",\n    eval_file=\"data/dev_simple.json\",\n    experiment_name=\"MobileBERT\",\n    model_args=mobilebert_tuned_args,\n    min_examples=5\n)","metadata":{"id":"BkzkjMK3bhA7","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.015798Z","iopub.status.idle":"2025-12-09T18:44:10.01601Z","shell.execute_reply.started":"2025-12-09T18:44:10.01591Z","shell.execute_reply":"2025-12-09T18:44:10.015919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_context = (\n    \"Natural Language Processing (NLP) is a field of Artificial Intelligence that focuses \"\n    \"on enabling computers to understand and generate human language.\"\n)\n\nto_predict = [\n    {\n        \"context\": custom_context,\n        \"qas\": [\n            {\n                \"id\": \"demo-1\",\n                \"question\": \"What does NLP focus on?\",\n            },\n            {\n                \"id\": \"demo-2\",\n                \"question\": \"NLP is a subfield of what broader area?\",\n            },\n        ],\n    }\n]\n\npredictions, raw_outputs = mobilebert_tuned_results['model'].predict(to_predict)\n\n# Create mappings for easy lookup\nprediction_map = {pred['id']: pred for pred in predictions}\nraw_output_map = {output['id']: output for output in raw_outputs}\n\n# Iterate through the original input structure to get the question text\nfor context_qa_block in to_predict:\n    for qa_input in context_qa_block[\"qas\"]:\n        qa_id = qa_input[\"id\"]\n        question = qa_input[\"question\"]\n\n        # Get the corresponding prediction and raw output\n        if qa_id in prediction_map:\n            prediction = prediction_map[qa_id]\n            # Take the first (most confident) answer from the list\n            answer = prediction[\"answer\"][0]\n\n            # Retrieve probability from raw_output_map\n            raw_output_entry = raw_output_map.get(qa_id)\n            probability = raw_output_entry['probability'][0] if raw_output_entry and 'probability' in raw_output_entry and raw_output_entry['probability'] else None\n\n            print(f\"Q ({qa_id}): {question}\")\n            # Print question, answer and score\n            if probability is not None:\n                print(f\"  -> Answer: {answer!r}, score={probability:.4f}\")\n            else:\n                print(f\"  -> Answer: {answer!r}, score=N/A\")\n        else:\n            print(f\"Q ({qa_id}): {question}\")\n            print(f\"  -> No prediction found for this question.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hyperparameter  Tuning - Doc Stride & Max Sequence Length","metadata":{"id":"2ff1ZjnzjsYA"}},{"cell_type":"code","source":"albert_tuned_args = bert_args.copy()\nalbert_tuned_args[\"output_dir\"] = \"outputs/albert_tuned-squad\"\nalbert_tuned_args[\"best_model_dir\"] = \"outputs/albert_tuned-squad/best_model\"\nalbert_tuned_args[\"num_train_epochs\"] = 3 # initially trained for 1 epoch\nalbert_tuned_args[\"doc_stride\"] = 256 # initially 128\nalbert_tuned_args[\"learning_rate\"] = 5e-5 # initially 3e-5\nalbert_tuned_args[\"max_seq_length\"] = 500 # initially 384","metadata":{"id":"MCzfmtqsCv99","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.016894Z","iopub.status.idle":"2025-12-09T18:44:10.017215Z","shell.execute_reply.started":"2025-12-09T18:44:10.017039Z","shell.execute_reply":"2025-12-09T18:44:10.017053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"albert_tuned_results = pipeline(\n    model_type=\"albert\",\n    model_name=\"albert-base-v2\",\n    experiment_name=\"ALBERT-base-v2\",\n    train_file=\"data/train_simple.json\",\n    eval_file=\"data/dev_simple.json\",\n    model_args=albert_tuned_args,\n    min_examples=5\n)","metadata":{"id":"IKs9S1LUCv9-","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.018415Z","iopub.status.idle":"2025-12-09T18:44:10.018688Z","shell.execute_reply.started":"2025-12-09T18:44:10.018561Z","shell.execute_reply":"2025-12-09T18:44:10.018574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_context = (\n    \"Natural Language Processing (NLP) is a field of Artificial Intelligence that focuses \"\n    \"on enabling computers to understand and generate human language.\"\n)\n\nto_predict = [\n    {\n        \"context\": custom_context,\n        \"qas\": [\n            {\n                \"id\": \"demo-1\",\n                \"question\": \"What does NLP focus on?\",\n            },\n            {\n                \"id\": \"demo-2\",\n                \"question\": \"NLP is a subfield of what broader area?\",\n            },\n        ],\n    }\n]\n\npredictions, raw_outputs = albert_tuned_results['model'].predict(to_predict)\n\n# Create mappings for easy lookup\nprediction_map = {pred['id']: pred for pred in predictions}\nraw_output_map = {output['id']: output for output in raw_outputs}\n\n# Iterate through the original input structure to get the question text\nfor context_qa_block in to_predict:\n    for qa_input in context_qa_block[\"qas\"]:\n        qa_id = qa_input[\"id\"]\n        question = qa_input[\"question\"]\n\n        # Get the corresponding prediction and raw output\n        if qa_id in prediction_map:\n            prediction = prediction_map[qa_id]\n            # Take the first (most confident) answer from the list\n            answer = prediction[\"answer\"][0]\n\n            # Retrieve probability from raw_output_map\n            raw_output_entry = raw_output_map.get(qa_id)\n            probability = raw_output_entry['probability'][0] if raw_output_entry and 'probability' in raw_output_entry and raw_output_entry['probability'] else None\n\n            print(f\"Q ({qa_id}): {question}\")\n            # Print question, answer and score\n            if probability is not None:\n                print(f\"  -> Answer: {answer!r}, score={probability:.4f}\")\n            else:\n                print(f\"  -> Answer: {answer!r}, score=N/A\")\n        else:\n            print(f\"Q ({qa_id}): {question}\")\n            print(f\"  -> No prediction found for this question.\")\n\n","metadata":{"id":"DTDf7-7uCv9-","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.020273Z","iopub.status.idle":"2025-12-09T18:44:10.020654Z","shell.execute_reply.started":"2025-12-09T18:44:10.020494Z","shell.execute_reply":"2025-12-09T18:44:10.020509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"tHEL39Yxj508","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 12. Results Comparison Table","metadata":{"id":"EZyCK6ubbj7P"}},{"cell_type":"code","source":"results_df = pd.DataFrame({\n    'Model': ['BERT-base', 'ALBERT-base-v2', 'MobileBERT', 'MobileBERT Tuned', 'ALBERT Tuned'],\n    'Exact Match (%)': [\n        (bert_results['eval_results']['correct'] / (bert_results['eval_results']['correct'] + bert_results['eval_results']['similar'] + bert_results['eval_results']['incorrect']) * 100),\n        (albert_results['eval_results']['correct'] / (albert_results['eval_results']['correct'] + albert_results['eval_results']['similar'] + albert_results['eval_results']['incorrect']) * 100),\n        (mobilebert_results['eval_results']['correct'] / (mobilebert_results['eval_results']['correct'] + mobilebert_results['eval_results']['similar'] + mobilebert_results['eval_results']['incorrect']) * 100),\n        (mobilebert_tuned_results['eval_results']['correct'] / (mobilebert_tuned_results['eval_results']['correct'] + mobilebert_tuned_results['eval_results']['similar'] + mobilebert_tuned_results['eval_results']['incorrect']) * 100),\n        (albert_tuned_results['eval_results']['correct'] / (albert_tuned_results['eval_results']['correct'] + albert_tuned_results['eval_results']['similar'] + albert_tuned_results['eval_results']['incorrect']) * 100)\n    ],\n    'F1 Score (%)': [\n        ((bert_results['eval_results']['correct'] + bert_results['eval_results']['similar']) / (bert_results['eval_results']['correct'] + bert_results['eval_results']['similar'] + bert_results['eval_results']['incorrect']) * 100),\n        ((albert_results['eval_results']['correct'] + albert_results['eval_results']['similar']) / (albert_results['eval_results']['correct'] + albert_results['eval_results']['similar'] + albert_results['eval_results']['incorrect']) * 100),\n        ((mobilebert_results['eval_results']['correct'] + mobilebert_results['eval_results']['similar']) / (mobilebert_results['eval_results']['correct'] + mobilebert_results['eval_results']['similar'] + mobilebert_results['eval_results']['incorrect']) * 100),\n        ((mobilebert_tuned_results['eval_results']['correct'] + mobilebert_tuned_results['eval_results']['similar']) / (mobilebert_tuned_results['eval_results']['correct'] + mobilebert_tuned_results['eval_results']['similar'] + mobilebert_tuned_results['eval_results']['incorrect']) * 100),\n        ((albert_tuned_results['eval_results']['correct'] + albert_tuned_results['eval_results']['similar']) / (albert_tuned_results['eval_results']['correct'] + albert_tuned_results['eval_results']['similar'] + albert_tuned_results['eval_results']['incorrect']) * 100)\n    ],\n    'Training Time (min)': [\n        bert_results['training_time']/60,\n        albert_results['training_time']/60,\n        mobilebert_results['training_time']/60,\n        mobilebert_tuned_results['training_time']/60,\n        albert_tuned_results['training_time']/60\n    ],\n    'Parameters': ['110M', '12M', '25M', '25M', '12M']\n})\n\nresults_df['Exact Match (%)'] = results_df['Exact Match (%)'].round(2)\nresults_df['F1 Score (%)'] = results_df['F1 Score (%)'].round(2)\nresults_df['Training Time (min)'] = results_df['Training Time (min)'].round(2)\n\nresults_df.sort_values(by='Exact Match (%)', ascending=False, inplace=True)","metadata":{"id":"yWanMksvbnMK","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.021835Z","iopub.status.idle":"2025-12-09T18:44:10.022135Z","shell.execute_reply.started":"2025-12-09T18:44:10.021983Z","shell.execute_reply":"2025-12-09T18:44:10.021996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_df","metadata":{"id":"VT6YkGZabxbY","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.024435Z","iopub.status.idle":"2025-12-09T18:44:10.024733Z","shell.execute_reply.started":"2025-12-09T18:44:10.024581Z","shell.execute_reply":"2025-12-09T18:44:10.024594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save results\nresults_df.to_csv('experiment_results.csv', index=False)","metadata":{"id":"4wEA4_ASb0uZ","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.025932Z","iopub.status.idle":"2025-12-09T18:44:10.02617Z","shell.execute_reply.started":"2025-12-09T18:44:10.02605Z","shell.execute_reply":"2025-12-09T18:44:10.026062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualization\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# EM comparison\naxes[0].bar(results_df['Model'], results_df['Exact Match (%)'], color='steelblue')\naxes[0].set_ylabel('Exact Match (%)')\naxes[0].set_title('Exact Match Comparison')\naxes[0].tick_params(axis='x', rotation=45)\naxes[0].grid(axis='y', alpha=0.3)\n\n# F1 comparison\naxes[1].bar(results_df['Model'], results_df['F1 Score (%)'], color='coral')\naxes[1].set_ylabel('F1 Score (%)')\naxes[1].set_title('F1 Score Comparison')\naxes[1].tick_params(axis='x', rotation=45)\naxes[1].grid(axis='y', alpha=0.3)\n\n# Training time comparison\naxes[2].bar(results_df['Model'], results_df['Training Time (min)'], color='lightgreen')\naxes[2].set_ylabel('Training Time (minutes)')\naxes[2].set_title('Training Time Comparison')\naxes[2].tick_params(axis='x', rotation=45)\naxes[2].grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()\n","metadata":{"id":"SOe5wZSOb3WL","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:44:10.029119Z","iopub.status.idle":"2025-12-09T18:44:10.029551Z","shell.execute_reply.started":"2025-12-09T18:44:10.029385Z","shell.execute_reply":"2025-12-09T18:44:10.0294Z"}},"outputs":[],"execution_count":null}]}